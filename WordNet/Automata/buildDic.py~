import os
from nltk.corpus import wordnet as wn
from nltk.corpus import wordnet_ic as wnic
from itertools import product
from itertools import groupby
from operator import itemgetter
from nltk.stem.lancaster import LancasterStemmer

def uniqifying(seq):
  seen = set()
  seen_add = seen.add
  return [ x for x in seq if x not in seen and not seen_add(x)]

def extractEngWords(line):
  line = line.strip()
  return line.split()

def extractTerminals(prog):
  subs = []

  #s = 'PROJECT(AtomicColSet(COL_FARE()), EXTRACT_ROW_MIN_F(COL_FARE(), AtomicRowPredSet(AtomicRowPred(EQ_DEPARTS(CITY(atlanta), ANY(), ANY(), ANY(), ANY()), EQ_ARRIVES(CITY(dallas), ANY(), ANY(), ANY(), ANY())))))'
  s = prog

  flag = False

  shortArray = []

  for c in s:
    if c.isupper():
      flag = True
      shortArray.append(c)
    else:
      if flag == True:
	temp = ''.join(shortArray)
	subs.append(temp)
	shortArray = []
	flag = False
	continue
      else:
	continue
  
  truncSubs = subs
  for ss in subs:
    #if ss.endswith('S'):
      #print ss + ' hehe'
    if len(ss) == 1:
      truncSubs.remove(ss)
    #if ss == 'ANY':
      #truncSubs.remove(ss)
    #if ss.startswith('_'):
      #truncSubs.remove(ss)
  return truncSubs

def compare(w1, w2):		## returns the 
  #lch_threshold = 2.26
  ss1 = wn.synsets(w1)
  ss2 = wn.synsets(w2)
  #return max(ss1.path_similarity(ss2) for (ss1, ss2) in product(s1, s2))
  lis = []
  for s1 in ss1:
    for s2 in ss2:
      try:
	lch = s1.path_similarity(s2)
      except:
	continue
      # The value to compare the LCH to was found empirically.
      # (The value is very application dependent. Experiment!)
      #if lch >= lch_threshold:
      lis.append((w1, w2, lch))
  if(lis == []):
    return (w1, w2, -1)
  toCheckNone = max(lis, key = itemgetter(2))
  if toCheckNone[2] == None:
    return (w1, w2, -1)
  return toCheckNone

mapping_frequency_D = {}
seeding_maps = []
fo = open("./seed_mappings", "r")
seedMapLine = fo.readline()
while(seedMapLine):
  seedMapLine = seedMapLine.rstrip()
  seeding_maps.append(seedMapLine)
  seedMapLine = fo.readline()
fo.close()
for m in seeding_maps:
  mapping_frequency_D[str(m)] = 1

stemming = LancasterStemmer()
stem_similarity = 1.01
simi_threshold = 0.199
benchMsFO = open("benchmarkSample.txt", "r")
num = 0
while(1):
  engLine = benchMsFO.readline()
  num += 1
  if not engLine:
    break
  print num
  print str(engLine.rstrip())
  words = extractEngWords(engLine)
  wo = []
  for w in words:
    wo.append(w.lower())
  words = wo
  
  prog = benchMsFO.readline()
  prog = prog.rstrip()
  print str(prog)
  probableTerm = extractTerminals(prog)
  
  maxSimiArrPerWord = []
  maxSimiArrPerTerm = []
  for t1 in probableTerm:
    ####print str(t1) + ' ---- Terminals'				## the terminals are output here
    t1File = open("./Terminals/" + str(t1), "a+")
    uWords = []
    l1 = t1File.readline()
    if not (l1):
      t1File.close()
      continue
    while(l1):
      l1 = l1.rstrip()
      l1 = l1.lower()
      uWords.append(l1)
      l1 = t1File.readline()
    
    stem_uWords = []
    for w2 in uWords:
      stem_uWords.append(str(stemming.stem(w2)))
    stem_uWords = uniqifying(stem_uWords)
    
    maxSimiArrPerWord = []
    val = ['crap', 'crap', 0]
    for w1 in words:
      ####print str(w1) + ' -------word from sentence'
      #if w1 in uWords:
	#continue
      val = [w1, w1, 0]
      if (str(stemming.stem(w1)) in stem_uWords):
	val = [w1, 'stemming', stem_similarity]
      for w2 in uWords:
	####print w2
	tempval = compare(w1, w2)
	####print str(tempval) + ' ******************** Map of word to word and similarity val'
	if val[2] < tempval[2]:
	  val[2] = tempval[2]
	  val[1] = w2
      if not (val[2] == 0):
	maxSimiArrPerWord.append(val)
    #if not (maxSimiArrPerWord == []):
      #wToAddToTerm = max(maxSimiArrPerWord, key = itemgetter(2))
      #t1File.write(str(wToAddToTerm[0]) + '\n')
    
    t1File.close()
    if not (maxSimiArrPerWord == []):
      wToAddToTerm = max(maxSimiArrPerWord, key = itemgetter(2))
      if wToAddToTerm[2] > simi_threshold:
	maxSimiArrPerTerm.append([wToAddToTerm[0], t1, wToAddToTerm[2]])
      ####print str(maxSimiArrPerWord) + ' ** ' + str(maxSimiArrPerTerm)
  
  #print 'options for additions -##**##- ' + str(maxSimiArrPerTerm)
  sameWords = [max(items) for key, items in groupby(maxSimiArrPerTerm, key = itemgetter(0))]
  print 'additions -**##**- ' + str(sameWords) + '\n'
  for addingHighProb in sameWords:
    termFile = open("./Terminals/" + str(addingHighProb[1]), "a+")
    map_format = str(addingHighProb[1]) + '$' + str(addingHighProb[0])
    if map_format in mapping_frequency_D:
      mapping_frequency_D[map_format] += 1
    else:
      mapping_frequency_D[map_format] = 1
    uWords = []
    l1 = termFile.readline()
    while(l1):
      l1 = l1.rstrip()
      l1 = l1.lower()
      uWords.append(l1)
      l1 = termFile.readline()
    
    ####print str(uWords) + ' -------- the uWords list i.e. Terminal words'
    
    if not (str(addingHighProb[0]) in uWords):
      termFile.write(str(addingHighProb[0]) + '\n')
    termFile.close()
  
  blankLine = benchMsFO.readline()

fo = open("map_frequency", "w")
for keys, values in mapping_frequency_D.iteritems():
  fo.write(str(keys) + '\t\t' + str(values) + '\n')
fo.close()